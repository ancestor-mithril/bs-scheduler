
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.5">
    
    
      
        <title>BS Scheduler</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.6a10b989.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#bs_scheduler" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="BS Scheduler" class="md-header__button md-logo" aria-label="BS Scheduler" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            BS Scheduler
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Home
            
          </span>
        </div>
      </div>
    </div>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="BS Scheduler" class="md-nav__button md-logo" aria-label="BS Scheduler" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    BS Scheduler
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Home
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler" class="md-nav__link">
    bs_scheduler
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler" class="md-nav__link">
    BSScheduler
  </a>
  
    <nav class="md-nav" aria-label="BSScheduler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.batch_size" class="md-nav__link">
    batch_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.finished" class="md-nav__link">
    finished
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.last_bs" class="md-nav__link">
    last_bs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.set_batch_size" class="md-nav__link">
    set_batch_size()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.BatchSizeManager" class="md-nav__link">
    BatchSizeManager
  </a>
  
    <nav class="md-nav" aria-label="BatchSizeManager">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BatchSizeManager.get_current_batch_size" class="md-nav__link">
    get_current_batch_size()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BatchSizeManager.set_batch_size" class="md-nav__link">
    set_batch_size()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.ChainedBSScheduler" class="md-nav__link">
    ChainedBSScheduler
  </a>
  
    <nav class="md-nav" aria-label="ChainedBSScheduler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ChainedBSScheduler.finished" class="md-nav__link">
    finished
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ChainedBSScheduler.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ChainedBSScheduler.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ChainedBSScheduler.step" class="md-nav__link">
    step()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.ConstantBS" class="md-nav__link">
    ConstantBS
  </a>
  
    <nav class="md-nav" aria-label="ConstantBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ConstantBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.CosineAnnealingBS" class="md-nav__link">
    CosineAnnealingBS
  </a>
  
    <nav class="md-nav" aria-label="CosineAnnealingBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.CosineAnnealingBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.CosineAnnealingBSWithWarmRestarts" class="md-nav__link">
    CosineAnnealingBSWithWarmRestarts
  </a>
  
    <nav class="md-nav" aria-label="CosineAnnealingBSWithWarmRestarts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.CosineAnnealingBSWithWarmRestarts.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.CyclicBS" class="md-nav__link">
    CyclicBS
  </a>
  
    <nav class="md-nav" aria-label="CyclicBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.CyclicBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.CyclicBS.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.CyclicBS.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.ExponentialBS" class="md-nav__link">
    ExponentialBS
  </a>
  
    <nav class="md-nav" aria-label="ExponentialBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ExponentialBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.IncreaseBSOnPlateau" class="md-nav__link">
    IncreaseBSOnPlateau
  </a>
  
    <nav class="md-nav" aria-label="IncreaseBSOnPlateau">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.IncreaseBSOnPlateau.in_cooldown" class="md-nav__link">
    in_cooldown
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.IncreaseBSOnPlateau.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.IncreaseBSOnPlateau.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.LambdaBS" class="md-nav__link">
    LambdaBS
  </a>
  
    <nav class="md-nav" aria-label="LambdaBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.LambdaBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.LambdaBS.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.LambdaBS.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.LinearBS" class="md-nav__link">
    LinearBS
  </a>
  
    <nav class="md-nav" aria-label="LinearBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.LinearBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.MultiStepBS" class="md-nav__link">
    MultiStepBS
  </a>
  
    <nav class="md-nav" aria-label="MultiStepBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.MultiStepBS.finished" class="md-nav__link">
    finished
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.MultiStepBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.MultiplicativeBS" class="md-nav__link">
    MultiplicativeBS
  </a>
  
    <nav class="md-nav" aria-label="MultiplicativeBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.MultiplicativeBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.MultiplicativeBS.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.MultiplicativeBS.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.OneCycleBS" class="md-nav__link">
    OneCycleBS
  </a>
  
    <nav class="md-nav" aria-label="OneCycleBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.OneCycleBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.PolynomialBS" class="md-nav__link">
    PolynomialBS
  </a>
  
    <nav class="md-nav" aria-label="PolynomialBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.PolynomialBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.SequentialBS" class="md-nav__link">
    SequentialBS
  </a>
  
    <nav class="md-nav" aria-label="SequentialBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.SequentialBS.finished" class="md-nav__link">
    finished
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.SequentialBS.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.SequentialBS.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.SequentialBS.step" class="md-nav__link">
    step()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.StepBS" class="md-nav__link">
    StepBS
  </a>
  
    <nav class="md-nav" aria-label="StepBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.StepBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="reference.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API Documentation
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="tutorials.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler" class="md-nav__link">
    bs_scheduler
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler" class="md-nav__link">
    BSScheduler
  </a>
  
    <nav class="md-nav" aria-label="BSScheduler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.batch_size" class="md-nav__link">
    batch_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.finished" class="md-nav__link">
    finished
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.last_bs" class="md-nav__link">
    last_bs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.set_batch_size" class="md-nav__link">
    set_batch_size()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BSScheduler.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.BatchSizeManager" class="md-nav__link">
    BatchSizeManager
  </a>
  
    <nav class="md-nav" aria-label="BatchSizeManager">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BatchSizeManager.get_current_batch_size" class="md-nav__link">
    get_current_batch_size()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.BatchSizeManager.set_batch_size" class="md-nav__link">
    set_batch_size()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.ChainedBSScheduler" class="md-nav__link">
    ChainedBSScheduler
  </a>
  
    <nav class="md-nav" aria-label="ChainedBSScheduler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ChainedBSScheduler.finished" class="md-nav__link">
    finished
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ChainedBSScheduler.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ChainedBSScheduler.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ChainedBSScheduler.step" class="md-nav__link">
    step()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.ConstantBS" class="md-nav__link">
    ConstantBS
  </a>
  
    <nav class="md-nav" aria-label="ConstantBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ConstantBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.CosineAnnealingBS" class="md-nav__link">
    CosineAnnealingBS
  </a>
  
    <nav class="md-nav" aria-label="CosineAnnealingBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.CosineAnnealingBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.CosineAnnealingBSWithWarmRestarts" class="md-nav__link">
    CosineAnnealingBSWithWarmRestarts
  </a>
  
    <nav class="md-nav" aria-label="CosineAnnealingBSWithWarmRestarts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.CosineAnnealingBSWithWarmRestarts.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.CyclicBS" class="md-nav__link">
    CyclicBS
  </a>
  
    <nav class="md-nav" aria-label="CyclicBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.CyclicBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.CyclicBS.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.CyclicBS.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.ExponentialBS" class="md-nav__link">
    ExponentialBS
  </a>
  
    <nav class="md-nav" aria-label="ExponentialBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.ExponentialBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.IncreaseBSOnPlateau" class="md-nav__link">
    IncreaseBSOnPlateau
  </a>
  
    <nav class="md-nav" aria-label="IncreaseBSOnPlateau">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.IncreaseBSOnPlateau.in_cooldown" class="md-nav__link">
    in_cooldown
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.IncreaseBSOnPlateau.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.IncreaseBSOnPlateau.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.LambdaBS" class="md-nav__link">
    LambdaBS
  </a>
  
    <nav class="md-nav" aria-label="LambdaBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.LambdaBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.LambdaBS.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.LambdaBS.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.LinearBS" class="md-nav__link">
    LinearBS
  </a>
  
    <nav class="md-nav" aria-label="LinearBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.LinearBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.MultiStepBS" class="md-nav__link">
    MultiStepBS
  </a>
  
    <nav class="md-nav" aria-label="MultiStepBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.MultiStepBS.finished" class="md-nav__link">
    finished
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.MultiStepBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.MultiplicativeBS" class="md-nav__link">
    MultiplicativeBS
  </a>
  
    <nav class="md-nav" aria-label="MultiplicativeBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.MultiplicativeBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.MultiplicativeBS.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.MultiplicativeBS.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.OneCycleBS" class="md-nav__link">
    OneCycleBS
  </a>
  
    <nav class="md-nav" aria-label="OneCycleBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.OneCycleBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.PolynomialBS" class="md-nav__link">
    PolynomialBS
  </a>
  
    <nav class="md-nav" aria-label="PolynomialBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.PolynomialBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.SequentialBS" class="md-nav__link">
    SequentialBS
  </a>
  
    <nav class="md-nav" aria-label="SequentialBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.SequentialBS.finished" class="md-nav__link">
    finished
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.SequentialBS.load_state_dict" class="md-nav__link">
    load_state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.SequentialBS.state_dict" class="md-nav__link">
    state_dict()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.SequentialBS.step" class="md-nav__link">
    step()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bs_scheduler.StepBS" class="md-nav__link">
    StepBS
  </a>
  
    <nav class="md-nav" aria-label="StepBS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bs_scheduler.StepBS.get_new_bs" class="md-nav__link">
    get_new_bs()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>Home</h1>

<div class="doc doc-object doc-module">



<a id="bs_scheduler"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.BSScheduler" class="doc doc-heading">
          <code>BSScheduler</code>


</h2>


  <div class="doc doc-contents ">


            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">BSScheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
                 <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Should we allow our users to use us with dataloader == None and just use the batch size managers they</span>
            <span class="c1"># provide us with?</span>
            <span class="n">check_isinstance</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Parameter dataloader is not a DataLoader. If you really need this feature, please open an issue at &quot;</span>
                  <span class="s2">&quot;https://github.com/ancestor-mithril/bs_scheduler/issues and describe your use case.&quot;</span><span class="p">)</span>
            <span class="k">raise</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span> <span class="o">=</span> <span class="n">dataloader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">verbose</span>

        <span class="k">assert</span> <span class="n">max_batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_batch_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">max_batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">max_batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">max_batch_size</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Maximum batch size must be greater than 0, but is </span><span class="si">{</span><span class="n">max_batch_size</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">max_batch_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span> <span class="n">max_batch_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">max_batch_size</span>

        <span class="k">if</span> <span class="n">min_batch_size</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Minimum batch size must be greater than 0, but is </span><span class="si">{</span><span class="n">min_batch_size</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">min_batch_size</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Minimum batch size must be smaller than or equal to the maximum batch size &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">max_batch_size</span><span class="si">}</span><span class="s2">), but is </span><span class="si">{</span><span class="n">min_batch_size</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">min_batch_size</span>

        <span class="k">if</span> <span class="n">batch_size_manager</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">batch_sampler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">batch_size_manager</span> <span class="o">=</span> <span class="n">DefaultBatchSizeManager</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># We require the client to implement a &quot;change_batch_size&quot; method and a &quot;get_batch_size&quot; method for</span>
                <span class="c1"># their dataset.</span>
                <span class="n">batch_size_manager</span> <span class="o">=</span> <span class="n">CustomBatchSizeManager</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">BatchSizeManager</span> <span class="o">=</span> <span class="n">batch_size_manager</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">,</span> <span class="s1">&#39;_base_batch_size&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">_base_batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_bs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">_base_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_get_new_bs</span><span class="p">()</span>

        <span class="c1"># Doing the zero-th step.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># The initial step may make the scheduler to finish during initialization. So we reinitialize self._finished.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">        It contains an entry for every variable in self.__dict__ which is not the dataloader.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span>
                <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;dataloader&#39;</span><span class="p">,</span> <span class="s1">&#39;_internal_get_new_bs&#39;</span><span class="p">)}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Loads the schedulers state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_bs</span><span class="p">)</span>  <span class="c1"># Setting the batch size to the last computed batch size.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_get_new_bs</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">set_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_bs</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Forwards the call for setting the new batch size to the batch size manager. If the dataloader batch_size</span>
<span class="sd">        member variable is not None, it also modifies it to reflect the change in batch size.</span>

<span class="sd">        Args:</span>
<span class="sd">            new_bs (int): The new batch sizes that needs to be set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># We can&#39;t directly do `dataloader.batch_size = new_bs` because the dataloader raises an error if we change</span>
            <span class="c1"># the batch size after initialization. But we are still hacking around it.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_bs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="o">.</span><span class="n">set_batch_size</span><span class="p">(</span><span class="n">new_bs</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the current batch size used by the dataloader as an :class:`int`. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="o">.</span><span class="n">get_current_batch_size</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">finished</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns True if the scheduler has already finished its job or has exceeded the minimum or maximum batch</span>
<span class="sd">        size. Otherwise, returns False.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">last_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the last computed batch size by current scheduler. If called before the first call to :meth:`step`</span>
<span class="sd">        returns the base batch size.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_bs</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Computes the next batch size. Should not be called explicitly in client code, but it doesn&#39;t really matter</span>
<span class="sd">        if the client does so. Some batch size schedulers use the keyword arguments.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_init_get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Setting the correct get_new_bs() dispatch function.</span>
        <span class="k">if</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_new_bs</span><span class="p">)</span><span class="o">.</span><span class="n">varkw</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_internal_get_new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_internal_bare_dispatch</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_internal_get_new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_internal_kwargs_dispatch</span>

    <span class="k">def</span> <span class="nf">_internal_bare_dispatch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_new_bs</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_internal_kwargs_dispatch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_new_bs</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">print_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_bs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Adjusting batch size to </span><span class="si">{</span><span class="n">new_bs</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># TODO: Documentation</span>
        <span class="c1"># TODO: Check how the dataloader behaves if we change the batch size mid epoch. Write a guideline for this.</span>
        <span class="c1">#  Changing the batch size does not impact batch sizes loaded by workers before the change.</span>
        <span class="c1"># TODO: Check if changing the batch size needs locking. Because of multiprocessing. Normally it should not.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
            <span class="k">return</span>  <span class="c1"># Stops doing work if already finished.</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_internal_get_new_bs</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span> <span class="o">&lt;=</span> <span class="n">new_bs</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">new_bs</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">new_bs</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_bs</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_batch_size</span><span class="p">(</span><span class="n">new_bs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">print_bs</span><span class="p">(</span><span class="n">new_bs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_bs</span> <span class="o">=</span> <span class="n">new_bs</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="bs_scheduler.BSScheduler.batch_size" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the current batch size used by the dataloader as an :class:<code>int</code>.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="bs_scheduler.BSScheduler.finished" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">finished</span><span class="p">:</span> <span class="nb">bool</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns True if the scheduler has already finished its job or has exceeded the minimum or maximum batch
size. Otherwise, returns False.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">




<h3 id="bs_scheduler.BSScheduler.last_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">last_bs</span><span class="p">:</span> <span class="nb">int</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the last computed batch size by current scheduler. If called before the first call to :meth:<code>step</code>
returns the base batch size.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.BSScheduler.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Computes the next batch size. Should not be called explicitly in client code, but it doesn't really matter
if the client does so. Some batch size schedulers use the keyword arguments.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Computes the next batch size. Should not be called explicitly in client code, but it doesn&#39;t really matter</span>
<span class="sd">    if the client does so. Some batch size schedulers use the keyword arguments.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.BSScheduler.load_state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Loads the schedulers state.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>state_dict</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>scheduler state. Should be an object returned from a call to :meth:<code>state_dict</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Loads the schedulers state.</span>

<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_bs</span><span class="p">)</span>  <span class="c1"># Setting the batch size to the last computed batch size.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_init_get_new_bs</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.BSScheduler.set_batch_size" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">set_batch_size</span><span class="p">(</span><span class="n">new_bs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Forwards the call for setting the new batch size to the batch size manager. If the dataloader batch_size
member variable is not None, it also modifies it to reflect the change in batch size.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>new_bs</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The new batch sizes that needs to be set.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_bs</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Forwards the call for setting the new batch size to the batch size manager. If the dataloader batch_size</span>
<span class="sd">    member variable is not None, it also modifies it to reflect the change in batch size.</span>

<span class="sd">    Args:</span>
<span class="sd">        new_bs (int): The new batch sizes that needs to be set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># We can&#39;t directly do `dataloader.batch_size = new_bs` because the dataloader raises an error if we change</span>
        <span class="c1"># the batch size after initialization. But we are still hacking around it.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_bs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="o">.</span><span class="n">set_batch_size</span><span class="p">(</span><span class="n">new_bs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.BSScheduler.state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">state_dict</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the state of the scheduler as a :class:<code>dict</code>.</p>
<p>It contains an entry for every variable in self.<strong>dict</strong> which is not the dataloader.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">    It contains an entry for every variable in self.__dict__ which is not the dataloader.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span>
            <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;dataloader&#39;</span><span class="p">,</span> <span class="s1">&#39;_internal_get_new_bs&#39;</span><span class="p">)}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.BatchSizeManager" class="doc doc-heading">
          <code>BatchSizeManager</code>


</h2>


  <div class="doc doc-contents ">

  
      <p>Base class for all batch size managers, used for getting and setting the batch size. It is not mandatory to
inherit from this, but users must implement :meth:<code>get_current_batch_size</code> and :meth:<code>set_batch_size</code>.</p>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">BatchSizeManager</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Base class for all batch size managers, used for getting and setting the batch size. It is not mandatory to</span>
<span class="sd">    inherit from this, but users must implement :meth:`get_current_batch_size` and :meth:`set_batch_size`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">get_current_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the current batch size used by the dataloader as an :class:`int`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">set_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_bs</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Sets the new value of the batch size.</span>

<span class="sd">        Args:</span>
<span class="sd">            new_bs (int): The new batch sizes that needs to be set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.BatchSizeManager.get_current_batch_size" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_current_batch_size</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the current batch size used by the dataloader as an :class:<code>int</code>.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_current_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the current batch size used by the dataloader as an :class:`int`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.BatchSizeManager.set_batch_size" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">set_batch_size</span><span class="p">(</span><span class="n">new_bs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Sets the new value of the batch size.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>new_bs</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The new batch sizes that needs to be set.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">set_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_bs</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Sets the new value of the batch size.</span>

<span class="sd">    Args:</span>
<span class="sd">        new_bs (int): The new batch sizes that needs to be set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.ChainedBSScheduler" class="doc doc-heading">
          <code>ChainedBSScheduler</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Similar to torch.optim.lr_scheduler.ChainedScheduler.
Chains a list of batch size schedulers. It takes the list of batch size schedulers and performs consucutive
step() functions belonging to them by just one call</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>schedulers</code></td>
          <td>
                <code><span title="typing.Sequence">Sequence</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>List of chained schedulers.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming the base batch size is 10.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 100 if epoch == 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 110 if epoch == 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 121 if epoch == 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 133 if epoch == 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 14 if epoch == 4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 15 if epoch == 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 16 if epoch == 6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 18 if epoch == 7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler1</span> <span class="o">=</span> <span class="n">ConstantBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">milestone</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler2</span> <span class="o">=</span> <span class="n">ExponentialBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">ChainedBSScheduler</span><span class="p">([</span><span class="n">scheduler1</span><span class="p">,</span> <span class="n">scheduler2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ChainedBSScheduler</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Similar to torch.optim.lr_scheduler.ChainedScheduler.</span>
<span class="sd">    Chains a list of batch size schedulers. It takes the list of batch size schedulers and performs consucutive</span>
<span class="sd">    step() functions belonging to them by just one call</span>

<span class="sd">    Args:</span>
<span class="sd">        schedulers (Sequence[BSScheduler]): List of chained schedulers.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; # Assuming the base batch size is 10.</span>
<span class="sd">        &gt;&gt;&gt; # bs = 100 if epoch == 0</span>
<span class="sd">        &gt;&gt;&gt; # bs = 110 if epoch == 1</span>
<span class="sd">        &gt;&gt;&gt; # bs = 121 if epoch == 2</span>
<span class="sd">        &gt;&gt;&gt; # bs = 133 if epoch == 3</span>
<span class="sd">        &gt;&gt;&gt; # bs = 14 if epoch == 4</span>
<span class="sd">        &gt;&gt;&gt; # bs = 15 if epoch == 5</span>
<span class="sd">        &gt;&gt;&gt; # bs = 16 if epoch == 6</span>
<span class="sd">        &gt;&gt;&gt; # bs = 18 if epoch == 7</span>
<span class="sd">        &gt;&gt;&gt; # ...</span>
<span class="sd">        &gt;&gt;&gt; scheduler1 = ConstantBS(dataloader, factor=10, milestone=4)</span>
<span class="sd">        &gt;&gt;&gt; scheduler2 = ExponentialBS(dataloader, gamma=1.1)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = ChainedBSScheduler([scheduler1, scheduler2])</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schedulers</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">BSScheduler</span><span class="p">]):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schedulers</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
            <span class="p">[</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">BSScheduler</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">schedulers</span><span class="p">])</span>

        <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span> <span class="o">=</span> <span class="n">schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dataloader</span>
        <span class="n">batch_size_manger</span><span class="p">:</span> <span class="n">BatchSizeManager</span> <span class="o">=</span> <span class="n">schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">batch_size_manager</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dataloader</span> <span class="o">!=</span> <span class="n">dataloader</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ChainedBSScheduler expects all schedulers to belong to the same dataloader, but got &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;scheduler at index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> to be different than the scheduler at index 0.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">batch_size_manger</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;ChainedBSScheduler expects all schedulers to have the same batch size manager, but got &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;scheduler at index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> to have a different batch size manager. Expected type of &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;batch size manager: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">batch_size_manger</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">, got: &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="c1"># We do not require equality for min_batch_size and max_batch_size, but maybe we should.</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span> <span class="o">=</span> <span class="n">dataloader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">BatchSizeManager</span> <span class="o">=</span> <span class="n">batch_size_manger</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">BSScheduler</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_bs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">last_bs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">min</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">min_batch_size</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># self.verbose: bool = False</span>
        <span class="c1"># self.last_epoch: int = 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_get_new_bs</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Executes the step() function for all schedulers in order.</span>

<span class="sd">        Args:</span>
<span class="sd">            **kwargs: All kwargs arguments are passed to each scheduler.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">scheduler</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">last_bs</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">finished</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns True if all the schedulers have already finished their job or have exceeded the minimum or maximum</span>
<span class="sd">        batch size. Otherwise, returns False.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="nb">all</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">finished</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">        It contains an entry for every variable in self.__dict__ which is not the dataloader. The wrapped scheduler</span>
<span class="sd">        states will also be saved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">):</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Loads the schedulers state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">schedulers</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;schedulers&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">schedulers</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">schedulers</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">set_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_bs</span><span class="p">)</span>  <span class="c1"># Setting the batch size to the last computed batch size.</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="bs_scheduler.ChainedBSScheduler.finished" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">finished</span><span class="p">:</span> <span class="nb">bool</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns True if all the schedulers have already finished their job or have exceeded the minimum or maximum
batch size. Otherwise, returns False.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.ChainedBSScheduler.load_state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Loads the schedulers state.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>state_dict</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>scheduler state. Should be an object returned from a call to :meth:<code>state_dict</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Loads the schedulers state.</span>

<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">schedulers</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;schedulers&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">schedulers</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">schedulers</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">set_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_bs</span><span class="p">)</span>  <span class="c1"># Setting the batch size to the last computed batch size.</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.ChainedBSScheduler.state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">state_dict</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the state of the scheduler as a :class:<code>dict</code>.</p>
<p>It contains an entry for every variable in self.<strong>dict</strong> which is not the dataloader. The wrapped scheduler
states will also be saved.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">    It contains an entry for every variable in self.__dict__ which is not the dataloader. The wrapped scheduler</span>
<span class="sd">    states will also be saved.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
    <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">):</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">state_dict</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.ChainedBSScheduler.step" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">step</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Executes the step() function for all schedulers in order.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>All kwargs arguments are passed to each scheduler.</p>
            </div>
          </td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Executes the step() function for all schedulers in order.</span>

<span class="sd">    Args:</span>
<span class="sd">        **kwargs: All kwargs arguments are passed to each scheduler.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">scheduler</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">:</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_last_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">last_bs</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.ConstantBS" class="doc doc-heading">
          <code>ConstantBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Increases the batch size by a constant multiplicative factor until the number of epochs reaches a pre-defined
milestone. The batch size is multiplied by the constant factor during initialization and is multiplied again with
the inverse of the constant factor when the milestone is reached.
If the constant factor makes the batch size increase the image out of bounds, the constant factor is changed
automatically such that the batch size remains within bounds.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>factor</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number we multiply the batch size until the milestone.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>milestone</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number of steps that the scheduler increases the learning rate. Default: 5.</p>
            </div>
          </td>
          <td>
                <code>5</code>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming the base batch size is 10.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 50 if epoch == 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 50 if epoch == 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 50 if epoch == 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 10 if epoch &gt;= 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">ConstantBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">milestone</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ConstantBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Increases the batch size by a constant multiplicative factor until the number of epochs reaches a pre-defined</span>
<span class="sd">    milestone. The batch size is multiplied by the constant factor during initialization and is multiplied again with</span>
<span class="sd">    the inverse of the constant factor when the milestone is reached.</span>
<span class="sd">    If the constant factor makes the batch size increase the image out of bounds, the constant factor is changed</span>
<span class="sd">    automatically such that the batch size remains within bounds.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        factor (float): The number we multiply the batch size until the milestone.</span>
<span class="sd">        milestone (int): The number of steps that the scheduler increases the learning rate. Default: 5.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; # Assuming the base batch size is 10.</span>
<span class="sd">        &gt;&gt;&gt; # bs = 50 if epoch == 0</span>
<span class="sd">        &gt;&gt;&gt; # bs = 50 if epoch == 1</span>
<span class="sd">        &gt;&gt;&gt; # bs = 50 if epoch == 2</span>
<span class="sd">        &gt;&gt;&gt; # bs = 10 if epoch &gt;= 3</span>
<span class="sd">        &gt;&gt;&gt; scheduler = ConstantBS(dataloader, factor=5, milestone=3)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">milestone</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">milestone</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">milestone</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">factor</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
        <span class="c1"># Factor is expected to be greater than 1.0, as this should be a warmup process.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">milestone</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">milestone</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. The value of the batch size is changed once at</span>
<span class="sd">        initialization, when the batch size is multiplied with the given factor, and twice when the milestone is</span>
<span class="sd">        reached and the batch size is multiplied with the inverse of the given factor. The factor is adjusted during</span>
<span class="sd">        initialization such that it does not return a batch size out of bounds.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">max_factor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="n">min_factor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">&gt;</span> <span class="n">max_factor</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">max_factor</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">&lt;</span> <span class="n">min_factor</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">min_factor</span>
            <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestone</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># My job is done.</span>
        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.ConstantBS.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. The value of the batch size is changed once at
initialization, when the batch size is multiplied with the given factor, and twice when the milestone is
reached and the batch size is multiplied with the inverse of the given factor. The factor is adjusted during
initialization such that it does not return a batch size out of bounds.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. The value of the batch size is changed once at</span>
<span class="sd">    initialization, when the batch size is multiplied with the given factor, and twice when the milestone is</span>
<span class="sd">    reached and the batch size is multiplied with the inverse of the given factor. The factor is adjusted during</span>
<span class="sd">    initialization such that it does not return a batch size out of bounds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">max_factor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="n">min_factor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">&gt;</span> <span class="n">max_factor</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">max_factor</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">&lt;</span> <span class="n">min_factor</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">min_factor</span>
        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestone</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># My job is done.</span>
    <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.CosineAnnealingBS" class="doc doc-heading">
          <code>CosineAnnealingBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Similar to torch.optim.lr_scheduler.CosineAnnealingLR which implements the cosine annealing part of
<code>SGDR: Stochastic Gradient Descent with Warm Restarts</code>_. For batch size, we perform reverse annealing and instead
of decreasing the batch size to min_batch_size we increase it to max_batch_size.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>total_iters</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number of steps that the scheduler increases the batch size.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>base_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The base batch size. If None, the base batch size will be retrieved from
the dataloader. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>
      <p>.. _SGDR\: Stochastic Gradient Descent with Warm Restarts: https://arxiv.org/abs/1608.03983</p>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming the base batch size is 10.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 10 if epoch % 10 == 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 19 if epoch % 10 == 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 41 if epoch % 10 == 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 69 if epoch % 10 == 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 91 if epoch % 10 == 4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 100 if epoch % 10 == 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 91 if epoch % 10 == 6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 67 if epoch % 10 == 7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 37 if epoch % 10 == 8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 13 if epoch % 10 == 9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineAnnealingBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">total_iters</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CosineAnnealingBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Similar to torch.optim.lr_scheduler.CosineAnnealingLR which implements the cosine annealing part of</span>
<span class="sd">    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. For batch size, we perform reverse annealing and instead</span>
<span class="sd">    of decreasing the batch size to min_batch_size we increase it to max_batch_size.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        total_iters (int): The number of steps that the scheduler increases the batch size.</span>
<span class="sd">        base_batch_size (Union[int, None]): The base batch size. If None, the base batch size will be retrieved from</span>
<span class="sd">            the dataloader. Default: None.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts: https://arxiv.org/abs/1608.03983</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; # Assuming the base batch size is 10.</span>
<span class="sd">        &gt;&gt;&gt; # bs = 10 if epoch % 10 == 0</span>
<span class="sd">        &gt;&gt;&gt; # bs = 19 if epoch % 10 == 1</span>
<span class="sd">        &gt;&gt;&gt; # bs = 41 if epoch % 10 == 2</span>
<span class="sd">        &gt;&gt;&gt; # bs = 69 if epoch % 10 == 3</span>
<span class="sd">        &gt;&gt;&gt; # bs = 91 if epoch % 10 == 4</span>
<span class="sd">        &gt;&gt;&gt; # bs = 100 if epoch % 10 == 5</span>
<span class="sd">        &gt;&gt;&gt; # bs = 91 if epoch % 10 == 6</span>
<span class="sd">        &gt;&gt;&gt; # bs = 67 if epoch % 10 == 7</span>
<span class="sd">        &gt;&gt;&gt; # bs = 37 if epoch % 10 == 8</span>
<span class="sd">        &gt;&gt;&gt; # bs = 13 if epoch % 10 == 9</span>
<span class="sd">        &gt;&gt;&gt; scheduler = CosineAnnealingBS(dataloader, total_iters=5)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">total_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">base_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">total_iters</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">total_iters</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="k">assert</span> <span class="n">base_batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">base_batch_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">base_batch_size</span> <span class="o">&gt;=</span> <span class="n">min_batch_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">total_iters</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">_base_batch_size</span> <span class="k">if</span> <span class="n">base_batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">base_batch_size</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_float_batch_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. Increases the batch size from base batch size to maximum</span>
<span class="sd">        batch size following a cyclic cosine curve. The implementation is equivalent to</span>
<span class="sd">        torch.optim.lr_scheduler.CosineAnnealingLR.get_lr() and instead of `eta_min` we use `self.max_batch_size` and</span>
<span class="sd">        we clip the values to be within bounds.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="n">new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                    <span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                    <span class="mi">1</span> <span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_bs</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span>
                    <span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">_float_batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_float_batch_size</span> <span class="o">=</span> <span class="n">new_bs</span>
        <span class="k">return</span> <span class="n">clip</span><span class="p">(</span><span class="n">rint</span><span class="p">(</span><span class="n">new_bs</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.CosineAnnealingBS.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. Increases the batch size from base batch size to maximum
batch size following a cyclic cosine curve. The implementation is equivalent to
torch.optim.lr_scheduler.CosineAnnealingLR.get_lr() and instead of <code>eta_min</code> we use <code>self.max_batch_size</code> and
we clip the values to be within bounds.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. Increases the batch size from base batch size to maximum</span>
<span class="sd">    batch size following a cyclic cosine curve. The implementation is equivalent to</span>
<span class="sd">    torch.optim.lr_scheduler.CosineAnnealingLR.get_lr() and instead of `eta_min` we use `self.max_batch_size` and</span>
<span class="sd">    we clip the values to be within bounds.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
        <span class="n">new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                <span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="k">elif</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                <span class="mi">1</span> <span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">new_bs</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span>
                <span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span>
                         <span class="bp">self</span><span class="o">.</span><span class="n">_float_batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_float_batch_size</span> <span class="o">=</span> <span class="n">new_bs</span>
    <span class="k">return</span> <span class="n">clip</span><span class="p">(</span><span class="n">rint</span><span class="p">(</span><span class="n">new_bs</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.CosineAnnealingBSWithWarmRestarts" class="doc doc-heading">
          <code>CosineAnnealingBSWithWarmRestarts</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Similar to torch.optim.lr_scheduler.CosineAnnealingWarmRestarts which implements <code>SGDR: Stochastic Gradient
Descent with Warm Restarts</code>_. Unlike torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, which decreases the
learning rate for :math:<code>t_{i}</code> iterations and then restarts, we increase the batch size from base_batch_size to
max_batch_size in :math:<code>t_{i} + 1</code> iterations, then the batch size is restarted.</p>
<p>This scheduler can be used after every batch.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>t_0</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number of iterations for the first restart is t_0 + 1.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>base_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The base batch size. If None, the base batch size will be retrieved from
the dataloader. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>factor</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The factor with which :math:<code>t_{i}</code> is increased after a restart. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>
      <p>.. _SGDR\: Stochastic Gradient Descent with Warm Restarts: https://arxiv.org/abs/1608.03983</p>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming the base batch size is 10.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 10 if last_epoch % 6 == 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 19 if last_epoch % 6 == 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 41 if last_epoch % 6 == 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 69 if last_epoch % 6 == 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 91 if last_epoch % 6 == 4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 100 if last_epoch % 6 == 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineAnnealingBSWithWarmRestarts</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">train_batch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CosineAnnealingBSWithWarmRestarts</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Similar to torch.optim.lr_scheduler.CosineAnnealingWarmRestarts which implements `SGDR: Stochastic Gradient</span>
<span class="sd">    Descent with Warm Restarts`_. Unlike torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, which decreases the</span>
<span class="sd">    learning rate for :math:`t_{i}` iterations and then restarts, we increase the batch size from base_batch_size to</span>
<span class="sd">    max_batch_size in :math:`t_{i} + 1` iterations, then the batch size is restarted.</span>

<span class="sd">    This scheduler can be used after every batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        t_0 (int): The number of iterations for the first restart is t_0 + 1.</span>
<span class="sd">        base_batch_size (Union[int, None]): The base batch size. If None, the base batch size will be retrieved from</span>
<span class="sd">            the dataloader. Default: None.</span>
<span class="sd">        factor (int): The factor with which :math:`t_{i}` is increased after a restart. Default: 1.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts: https://arxiv.org/abs/1608.03983</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; # Assuming the base batch size is 10.</span>
<span class="sd">        &gt;&gt;&gt; # bs = 10 if last_epoch % 6 == 0</span>
<span class="sd">        &gt;&gt;&gt; # bs = 19 if last_epoch % 6 == 1</span>
<span class="sd">        &gt;&gt;&gt; # bs = 41 if last_epoch % 6 == 2</span>
<span class="sd">        &gt;&gt;&gt; # bs = 69 if last_epoch % 6 == 3</span>
<span class="sd">        &gt;&gt;&gt; # bs = 91 if last_epoch % 6 == 4</span>
<span class="sd">        &gt;&gt;&gt; # bs = 100 if last_epoch % 6 == 5</span>
<span class="sd">        &gt;&gt;&gt; scheduler = CosineAnnealingBSWithWarmRestarts(dataloader, 10)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     for batch in dataloader:</span>
<span class="sd">        &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">        &gt;&gt;&gt;         scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">t_0</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">base_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t_0</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">t_0</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">factor</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">factor</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">base_batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">base_batch_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">base_batch_size</span> <span class="o">&gt;=</span> <span class="n">min_batch_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_0</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">t_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_i</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">t_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_cur</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">factor</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">_base_batch_size</span> <span class="k">if</span> <span class="n">base_batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">base_batch_size</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. Increases the batch size from base batch size to maximum</span>
<span class="sd">        batch and restarts. The implementation is similar to</span>
<span class="sd">        torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, but instead of `eta_min` we use max_batch_size, and we</span>
<span class="sd">        increase the batch size instead of decreasing the learning rate. We clip the values to always remain within</span>
<span class="sd">        bound.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Don&#39;t do anything at initialization.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_cur</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_cur</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_i</span><span class="p">:</span>  <span class="c1"># &gt; so that we reach max_batch_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_cur</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_i</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># + 1 so that we go back to base_batch_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_i</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span>

        <span class="n">new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                <span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_cur</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_i</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="n">clip</span><span class="p">(</span><span class="n">rint</span><span class="p">(</span><span class="n">new_bs</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.CosineAnnealingBSWithWarmRestarts.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. Increases the batch size from base batch size to maximum
batch and restarts. The implementation is similar to
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, but instead of <code>eta_min</code> we use max_batch_size, and we
increase the batch size instead of decreasing the learning rate. We clip the values to always remain within
bound.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. Increases the batch size from base batch size to maximum</span>
<span class="sd">    batch and restarts. The implementation is similar to</span>
<span class="sd">    torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, but instead of `eta_min` we use max_batch_size, and we</span>
<span class="sd">    increase the batch size instead of decreasing the learning rate. We clip the values to always remain within</span>
<span class="sd">    bound.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Don&#39;t do anything at initialization.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">t_cur</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_cur</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_i</span><span class="p">:</span>  <span class="c1"># &gt; so that we reach max_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_cur</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_i</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># + 1 so that we go back to base_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_i</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span>

    <span class="n">new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
            <span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_cur</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_i</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">clip</span><span class="p">(</span><span class="n">rint</span><span class="p">(</span><span class="n">new_bs</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.CyclicBS" class="doc doc-heading">
          <code>CyclicBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Similar to torch.optim.lr_scheduler.CyclicLR. Sets the batch size according to a cyclical batch size policy,
inspired from the cyclical learning rate policy (CLR). The policy cycles the batch size between two boundaries with
a constant frequency, similar to a reversed cycle from the method detailed in the paper <code>Cyclical Learning Rates
for Training Neural Networks</code>_. The distance between the two boundaries can be scaled on a per-iteration or
per-cycle basis.</p>
<p>Cyclical batch size policy changes the batch size after every batch. The step() function should be called after a
batch has been used for training.</p>
<p>This class has three built-in policies, as put forth in the paper:</p>
<ul>
<li>"triangular": A basic triangular cycle without amplitude scaling.</li>
<li>"triangular2": A basic triangular cycle that scales initial amplitude by half each cycle.</li>
<li>"exp_range": A cycle that scales initial amplitude by :math:<code>\gamma^{\text{cycle iterations}}</code> at each cycle
    iteration.</li>
</ul>
<p>This implementation was adapted from <code>pytorch/pytorch</code><em> which was adapted from the github repo: <code>bckenstler/CLR</code></em>.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>base_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Initial batch size which is the lower boundery in the cycle. If None, the
base batch size will be retrieved from the dataloader. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>step_size_down</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of training iterations in the decreasing half of a cycle. Default: 2000.</p>
            </div>
          </td>
          <td>
                <code>2000</code>
          </td>
        </tr>
        <tr>
          <td><code>step_size_up</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of training iterations in the increasing half of a cycle. If
step_size_down is None, it is set to step_size_down. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>One of <code>triangular</code>, <code>triangular2</code>, <code>exp_range</code>. Values correspond to the policies detailed above.
If scale_fn is not None, this argument is ignored. Default: 'triangular'.</p>
            </div>
          </td>
          <td>
                <code>&#39;triangular&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>gamma</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Constant in the 'exp_range' scaling function: gamma ** (cycle iterations). Default: 1.0.</p>
            </div>
          </td>
          <td>
                <code>1.0</code>
          </td>
        </tr>
        <tr>
          <td><code>scale_fn</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.Callable">Callable</span>[[int], float], None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Custom scaling policy defined by a single argument lambda
function, where 0 &lt;= scale_fn(x) &lt;= 1 for all x &gt;= 0. If specified, then 'mode' is ignored. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>scale_mode</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>One of <code>cycle</code>, <code>iterations</code>. Defines whether scale_fn is evaluated on cycle number of cycle
iterations (training iterations since the start of the cycle). When scale_fn is None, scale_mode is
automatically set to 'iterations' if mode is 'exp_range' and 'cycle' otherwhise. Default: 'cycle'.</p>
            </div>
          </td>
          <td>
                <code>&#39;cycle&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper batch size boundary in the cycle. Functionally, it defines the cycle
amplitude (upper_batch_size_bound - base_batch_size). The batch size at any cycle is the sum of
base_batch_size and some scaling of the amplitude; therefore, upper_batch_size_bound may not actually be
reached depending on scaling function. If None or greater than the lenght of the dataset wrapped by the
dataloader, max_batch_size is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">CyclicBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">train_batch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>
      <p>.. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186
.. _pytorch/pytorch: https://github.com/pytorch/pytorch
.. _bckenstler/CLR: https://github.com/bckenstler/CLR</p>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CyclicBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Similar to torch.optim.lr_scheduler.CyclicLR. Sets the batch size according to a cyclical batch size policy,</span>
<span class="sd">    inspired from the cyclical learning rate policy (CLR). The policy cycles the batch size between two boundaries with</span>
<span class="sd">    a constant frequency, similar to a reversed cycle from the method detailed in the paper `Cyclical Learning Rates</span>
<span class="sd">    for Training Neural Networks`_. The distance between the two boundaries can be scaled on a per-iteration or</span>
<span class="sd">    per-cycle basis.</span>

<span class="sd">    Cyclical batch size policy changes the batch size after every batch. The step() function should be called after a</span>
<span class="sd">    batch has been used for training.</span>

<span class="sd">    This class has three built-in policies, as put forth in the paper:</span>

<span class="sd">    * &quot;triangular&quot;: A basic triangular cycle without amplitude scaling.</span>
<span class="sd">    * &quot;triangular2&quot;: A basic triangular cycle that scales initial amplitude by half each cycle.</span>
<span class="sd">    * &quot;exp_range&quot;: A cycle that scales initial amplitude by :math:`\\gamma^{\\text{cycle iterations}}` at each cycle</span>
<span class="sd">        iteration.</span>

<span class="sd">    This implementation was adapted from `pytorch/pytorch`_ which was adapted from the github repo: `bckenstler/CLR`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        base_batch_size (Union[int, None]): Initial batch size which is the lower boundery in the cycle. If None, the</span>
<span class="sd">            base batch size will be retrieved from the dataloader. Default: None.</span>
<span class="sd">        step_size_down (int): Number of training iterations in the decreasing half of a cycle. Default: 2000.</span>
<span class="sd">        step_size_up (Union[int, None]): Number of training iterations in the increasing half of a cycle. If</span>
<span class="sd">            step_size_down is None, it is set to step_size_down. Default: None.</span>
<span class="sd">        mode (str): One of `triangular`, `triangular2`, `exp_range`. Values correspond to the policies detailed above.</span>
<span class="sd">            If scale_fn is not None, this argument is ignored. Default: &#39;triangular&#39;.</span>
<span class="sd">        gamma (float): Constant in the &#39;exp_range&#39; scaling function: gamma ** (cycle iterations). Default: 1.0.</span>
<span class="sd">        scale_fn (Union[Callable[[int], float], None]): Custom scaling policy defined by a single argument lambda</span>
<span class="sd">            function, where 0 &lt;= scale_fn(x) &lt;= 1 for all x &gt;= 0. If specified, then &#39;mode&#39; is ignored. Default: None.</span>
<span class="sd">        scale_mode (str): One of `cycle`, `iterations`. Defines whether scale_fn is evaluated on cycle number of cycle</span>
<span class="sd">            iterations (training iterations since the start of the cycle). When scale_fn is None, scale_mode is</span>
<span class="sd">            automatically set to &#39;iterations&#39; if mode is &#39;exp_range&#39; and &#39;cycle&#39; otherwhise. Default: &#39;cycle&#39;.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper batch size boundary in the cycle. Functionally, it defines the cycle</span>
<span class="sd">            amplitude (upper_batch_size_bound - base_batch_size). The batch size at any cycle is the sum of</span>
<span class="sd">            base_batch_size and some scaling of the amplitude; therefore, upper_batch_size_bound may not actually be</span>
<span class="sd">            reached depending on scaling function. If None or greater than the lenght of the dataset wrapped by the</span>
<span class="sd">            dataloader, max_batch_size is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; scheduler = CyclicBS(dataloader)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     for batch in dataloader:</span>
<span class="sd">        &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">        &gt;&gt;&gt;         scheduler.step()</span>

<span class="sd">    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186</span>
<span class="sd">    .. _pytorch/pytorch: https://github.com/pytorch/pytorch</span>
<span class="sd">    .. _bckenstler/CLR: https://github.com/bckenstler/CLR</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">base_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">step_size_down</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">step_size_up</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;triangular&#39;</span><span class="p">,</span>
                 <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">scale_fn</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">scale_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;cycle&#39;</span><span class="p">,</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">base_batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">base_batch_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">base_batch_size</span> <span class="o">&gt;=</span> <span class="n">min_batch_size</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">step_size_down</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">step_size_down</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">step_size_up</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">step_size_up</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">step_size_up</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span> <span class="ow">and</span> <span class="n">gamma</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
        <span class="k">assert</span> <span class="n">scale_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">callable</span><span class="p">(</span><span class="n">scale_fn</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">scale_mode</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;cycle&#39;</span><span class="p">,</span> <span class="s1">&#39;iterations&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;triangular&#39;</span><span class="p">,</span> <span class="s1">&#39;triangular2&#39;</span><span class="p">,</span> <span class="s1">&#39;exp_range&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">scale_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;CyclicBS requires either a valid mode or passing a custom scale_fn.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">mode</span>

        <span class="k">if</span> <span class="n">step_size_up</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">step_size_up</span> <span class="o">=</span> <span class="n">step_size_down</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">step_size_down</span> <span class="o">+</span> <span class="n">step_size_up</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">step_size_down</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_custom</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="n">scale_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">scale_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_scale_fn</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="n">base_batch_size</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">_base_batch_size</span> <span class="k">if</span> <span class="n">base_batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">base_batch_size</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span>

    <span class="k">def</span> <span class="nf">_init_scale_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_custom</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_custom</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;triangular&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_triangular_scale_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">=</span> <span class="s1">&#39;cycle&#39;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;triangular2&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_triangular2_scale_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">=</span> <span class="s1">&#39;cycle&#39;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;exp_range&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_exp_range_scale_fn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">=</span> <span class="s1">&#39;iterations&#39;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_triangular_scale_fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1.0</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_triangular2_scale_fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">**</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_exp_range_scale_fn</span><span class="p">(</span><span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">gamma</span> <span class="o">**</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. The value of the batch size cycles from base_batch_size to</span>
<span class="sd">        max_batch_size and back, while being scaled at each iteration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Return base batch size or current batch size at initialization.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

        <span class="n">ratio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span>
        <span class="n">cycle</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">ratio</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">ratio</span> <span class="o">-</span> <span class="n">cycle</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span><span class="p">:</span>
            <span class="n">scale_factor</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scale_factor</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">base_height</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factor</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">==</span> <span class="s1">&#39;cycle&#39;</span><span class="p">:</span>
            <span class="n">base_height</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span><span class="n">cycle</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">base_height</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">-</span> <span class="n">base_height</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">        It contains an entry for every variable in self.__dict__ which is not the dataloader. The wrapped scheduler</span>
<span class="sd">        states will also be saved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;scale_fn&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_custom</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;_scale_fn_custom&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Loads the schedulers state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_scale_fn</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.CyclicBS.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. The value of the batch size cycles from base_batch_size to
max_batch_size and back, while being scaled at each iteration.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. The value of the batch size cycles from base_batch_size to</span>
<span class="sd">    max_batch_size and back, while being scaled at each iteration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Return base batch size or current batch size at initialization.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="n">ratio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span>
    <span class="n">cycle</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">ratio</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">ratio</span> <span class="o">-</span> <span class="n">cycle</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scale_factor</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">base_height</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factor</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">==</span> <span class="s1">&#39;cycle&#39;</span><span class="p">:</span>
        <span class="n">base_height</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span><span class="n">cycle</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">base_height</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span> <span class="o">-</span> <span class="n">base_height</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.CyclicBS.load_state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Loads the schedulers state.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>state_dict</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>scheduler state. Should be an object returned from a call to :meth:<code>state_dict</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Loads the schedulers state.</span>

<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_init_scale_fn</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.CyclicBS.state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">state_dict</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the state of the scheduler as a :class:<code>dict</code>.</p>
<p>It contains an entry for every variable in self.<strong>dict</strong> which is not the dataloader. The wrapped scheduler
states will also be saved.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">    It contains an entry for every variable in self.__dict__ which is not the dataloader. The wrapped scheduler</span>
<span class="sd">    states will also be saved.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
    <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;scale_fn&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale_fn_custom</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;_scale_fn_custom&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">state_dict</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.ExponentialBS" class="doc doc-heading">
          <code>ExponentialBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Increases the batch size by a gamma every epoch.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>gamma</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Multiplicative factor of batch size growth.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming the base batch size is 10.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 10 if epoch == 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 11 if epoch == 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 12 if epoch == 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 13 if epoch == 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ExponentialBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Increases the batch size by a gamma every epoch.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        gamma (float): Multiplicative factor of batch size growth.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; # Assuming the base batch size is 10.</span>
<span class="sd">        &gt;&gt;&gt; # bs = 10 if epoch == 0</span>
<span class="sd">        &gt;&gt;&gt; # bs = 11 if epoch == 1</span>
<span class="sd">        &gt;&gt;&gt; # bs = 12 if epoch == 2</span>
<span class="sd">        &gt;&gt;&gt; # bs = 13 if epoch == 3</span>
<span class="sd">        &gt;&gt;&gt; # ...</span>
<span class="sd">        &gt;&gt;&gt; scheduler = ExponentialBS(dataloader, gamma=1.1)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">gamma</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
        <span class="c1"># Gamma is expected to be greater than 1.0 for batch size growth. It can be lower than 1.0 for batch size decay.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. The current batch size is multiplied by gamma each epoch</span>
<span class="sd">        except the first one.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.ExponentialBS.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. The current batch size is multiplied by gamma each epoch
except the first one.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. The current batch size is multiplied by gamma each epoch</span>
<span class="sd">    except the first one.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.IncreaseBSOnPlateau" class="doc doc-heading">
          <code>IncreaseBSOnPlateau</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>The inverse of torch.optim.lr_scheduler.ReduceLROnPlateau.
Increases the batch size when a metric has stopped improving. Models often benefit from increasing the batch size
by a factor once the learning stagnates. This scheduler receives a metric value and if no improvement is seen for a
given number of epochs, the batch size is increased.
Unfortunately, this class is not compatible with the other batch size schedulers as its step() function needs to
receive the metric value.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>One of <code>min</code>, <code>max</code>. In <code>min</code> mode, the batch size will be increased when the metric value has
stopped decreasing; in <code>max</code> mode, the batch size will be increased when the metric value has stopped
increasing. Default: 'min'.</p>
            </div>
          </td>
          <td>
                <code>&#39;min&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>factor</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Factor by which the batch size will be increased. Default: 2.0.</p>
            </div>
          </td>
          <td>
                <code>2.0</code>
          </td>
        </tr>
        <tr>
          <td><code>patience</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of epochs with no improvement after which the batch size will be increased. Default: 10.</p>
            </div>
          </td>
          <td>
                <code>10</code>
          </td>
        </tr>
        <tr>
          <td><code>threshold</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Threshold for measuring the new metric value, to only focus on significant changes.
Default: 1e-4.</p>
            </div>
          </td>
          <td>
                <code>0.0001</code>
          </td>
        </tr>
        <tr>
          <td><code>threshold_mode</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>One of <code>rel</code>, <code>abs</code>. In <code>rel</code> mode, dynamic_threshold = best * ( 1 + threshold ) in 'max'
mode or best * ( 1 - threshold ) in <code>min</code> mode. In <code>abs</code> mode, dynamic_threshold = best + threshold in 'max'
mode or best - threshold in <code>min</code> mode. Default: 'rel'.</p>
            </div>
          </td>
          <td>
                <code>&#39;rel&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>cooldown</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of epochs to wait before resuming normal operation after the batch size has been reduced.
Default: 0.</p>
            </div>
          </td>
          <td>
                <code>0</code>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">IncreaseBSOnPlateau</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">val_loss</span><span class="p">)</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">IncreaseBSOnPlateau</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; The inverse of torch.optim.lr_scheduler.ReduceLROnPlateau.</span>
<span class="sd">    Increases the batch size when a metric has stopped improving. Models often benefit from increasing the batch size</span>
<span class="sd">    by a factor once the learning stagnates. This scheduler receives a metric value and if no improvement is seen for a</span>
<span class="sd">    given number of epochs, the batch size is increased.</span>
<span class="sd">    Unfortunately, this class is not compatible with the other batch size schedulers as its step() function needs to</span>
<span class="sd">    receive the metric value.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        mode (str): One of `min`, `max`. In `min` mode, the batch size will be increased when the metric value has</span>
<span class="sd">            stopped decreasing; in `max` mode, the batch size will be increased when the metric value has stopped</span>
<span class="sd">            increasing. Default: &#39;min&#39;.</span>
<span class="sd">        factor (float): Factor by which the batch size will be increased. Default: 2.0.</span>
<span class="sd">        patience (int): Number of epochs with no improvement after which the batch size will be increased. Default: 10.</span>
<span class="sd">        threshold (float): Threshold for measuring the new metric value, to only focus on significant changes.</span>
<span class="sd">            Default: 1e-4.</span>
<span class="sd">        threshold_mode (str): One of `rel`, `abs`. In `rel` mode, dynamic_threshold = best * ( 1 + threshold ) in &#39;max&#39;</span>
<span class="sd">            mode or best * ( 1 - threshold ) in `min` mode. In `abs` mode, dynamic_threshold = best + threshold in &#39;max&#39;</span>
<span class="sd">            mode or best - threshold in `min` mode. Default: &#39;rel&#39;.</span>
<span class="sd">        cooldown (int): Number of epochs to wait before resuming normal operation after the batch size has been reduced.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; scheduler = IncreaseBSOnPlateau(dataloader)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     val_loss = validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step(metric=val_loss)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">patience</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                 <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;rel&#39;</span><span class="p">,</span> <span class="n">cooldown</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">factor</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span> <span class="ow">and</span> <span class="n">factor</span> <span class="o">!=</span> <span class="mf">1.0</span> <span class="ow">and</span> <span class="n">factor</span> <span class="o">&gt;=</span> <span class="mf">0.0</span>
        <span class="c1"># Factor is expected to be greater than 1, but we do not forbid batch size decay.</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">patience</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">patience</span> <span class="o">&gt;=</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span> <span class="ow">and</span> <span class="n">threshold</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cooldown</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">cooldown</span> <span class="o">&gt;=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">factor</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">patience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">threshold</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">threshold_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cooldown</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">cooldown</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode_worse</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inf</span> <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;min&#39;</span> <span class="k">else</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode_worse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># setting last epoch to 0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_is_better</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">in_cooldown</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns True if scheduler is in cooldown, False otherwise.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Resets num_bad_epochs counter and cooldown counter.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode_worse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">is_better_min_rel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">best</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">best</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">threshold</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">is_better_min_abs</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">best</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">best</span> <span class="o">-</span> <span class="n">threshold</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">is_better_max_rel</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">best</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">best</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">threshold</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">is_better_max_abs</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">best</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">best</span> <span class="o">+</span> <span class="n">threshold</span>

    <span class="k">def</span> <span class="nf">_init_is_better</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mode </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s1"> is unknown!&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">threshold_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;rel&#39;</span><span class="p">,</span> <span class="s1">&#39;abs&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Threshold mode </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s1"> is unknown!&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;min&#39;</span> <span class="ow">and</span> <span class="n">threshold_mode</span> <span class="o">==</span> <span class="s1">&#39;rel&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_better</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_better_min_rel</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;min&#39;</span> <span class="ow">and</span> <span class="n">threshold_mode</span> <span class="o">==</span> <span class="s1">&#39;abs&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_better</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_better_min_abs</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;max&#39;</span> <span class="ow">and</span> <span class="n">threshold_mode</span> <span class="o">==</span> <span class="s1">&#39;rel&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_better</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_better_max_rel</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># mode == &#39;min&#39; and threshold_mode == &#39;abs&#39;:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_better</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_better_max_abs</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. Receives a metric and increases the batch size by a give</span>
<span class="sd">        factor if the metric has not been improved for `patience` epochs. After increasing the batch size, the</span>
<span class="sd">        scheduler goes through a cooldown period in which bad epochs are ignored.</span>

<span class="sd">        Args:</span>
<span class="sd">            **kwargs: All keyword arguments except &#39;metric&#39; are ignored. The keyword &#39;metric&#39; must be passed to the</span>
<span class="sd">                step() function, otherwise a TypeError would be raised.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Don&#39;t do anything at initialization.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

        <span class="n">metric</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">metric</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;IncreaseBSOnPlateau requires passing a &#39;metric&#39; keyword argument in the step() function.&quot;</span><span class="p">)</span>

        <span class="n">current</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">metric</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_better</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">best</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best</span> <span class="o">=</span> <span class="n">current</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_cooldown</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># ignore any bad epochs in cooldown.</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cooldown</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Loads the schedulers state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_is_better</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="bs_scheduler.IncreaseBSOnPlateau.in_cooldown" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">in_cooldown</span><span class="p">:</span> <span class="nb">bool</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns True if scheduler is in cooldown, False otherwise.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.IncreaseBSOnPlateau.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. Receives a metric and increases the batch size by a give
factor if the metric has not been improved for <code>patience</code> epochs. After increasing the batch size, the
scheduler goes through a cooldown period in which bad epochs are ignored.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>All keyword arguments except 'metric' are ignored. The keyword 'metric' must be passed to the
step() function, otherwise a TypeError would be raised.</p>
            </div>
          </td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. Receives a metric and increases the batch size by a give</span>
<span class="sd">    factor if the metric has not been improved for `patience` epochs. After increasing the batch size, the</span>
<span class="sd">    scheduler goes through a cooldown period in which bad epochs are ignored.</span>

<span class="sd">    Args:</span>
<span class="sd">        **kwargs: All keyword arguments except &#39;metric&#39; are ignored. The keyword &#39;metric&#39; must be passed to the</span>
<span class="sd">            step() function, otherwise a TypeError would be raised.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Don&#39;t do anything at initialization.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="n">metric</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">metric</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;IncreaseBSOnPlateau requires passing a &#39;metric&#39; keyword argument in the step() function.&quot;</span><span class="p">)</span>

    <span class="n">current</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">metric</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_better</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">best</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best</span> <span class="o">=</span> <span class="n">current</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_cooldown</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># ignore any bad epochs in cooldown.</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cooldown</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.IncreaseBSOnPlateau.load_state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Loads the schedulers state.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>state_dict</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>scheduler state. Should be an object returned from a call to :meth:<code>state_dict</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Loads the schedulers state.</span>

<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_init_is_better</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.LambdaBS" class="doc doc-heading">
          <code>LambdaBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Sets the batch size to the initial batch size times a given function. Unlike torch.optim.lr_scheduler.LambdaLR,
there is a single batch size for a given dataloader so only one function should be passed as a parameter.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>bs_lambda</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span>[[int], float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A function which computes a multiplicative factor given an integer
parameter epoch.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mf">1.05</span> <span class="o">**</span> <span class="n">epoch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">LambdaBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">bs_lambda</span><span class="o">=</span><span class="n">func</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LambdaBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Sets the batch size to the initial batch size times a given function. Unlike torch.optim.lr_scheduler.LambdaLR,</span>
<span class="sd">    there is a single batch size for a given dataloader so only one function should be passed as a parameter.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        bs_lambda (Callable[[int], float]): A function which computes a multiplicative factor given an integer</span>
<span class="sd">            parameter epoch.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; func = lambda epoch: 1.05 ** epoch</span>
<span class="sd">        &gt;&gt;&gt; scheduler = LambdaBS(dataloader, bs_lambda=func)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">bs_lambda</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">callable</span><span class="p">(</span><span class="n">bs_lambda</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">bs_lambda</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">        It contains an entry for every variable in self.__dict__ which is not the dataloader. The batch size lambda</span>
<span class="sd">        function will only be saved if they are callable objects and not if they are functions or lambdas.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">):</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads the schedulers state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bs_lambda</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bs_lambda</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">bs_lambda</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. It is calculated as the initial value of the batch size</span>
<span class="sd">        times the factor returned by `bs_lambda`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">_base_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.LambdaBS.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. It is calculated as the initial value of the batch size
times the factor returned by <code>bs_lambda</code>.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. It is calculated as the initial value of the batch size</span>
<span class="sd">    times the factor returned by `bs_lambda`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">_base_batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.LambdaBS.load_state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Loads the schedulers state.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>state_dict</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>scheduler state. Should be an object returned from a call to :meth:<code>state_dict</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads the schedulers state.</span>

<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bs_lambda</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bs_lambda</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">bs_lambda</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.LambdaBS.state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">state_dict</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the state of the scheduler as a :class:<code>dict</code>.</p>
<p>It contains an entry for every variable in self.<strong>dict</strong> which is not the dataloader. The batch size lambda
function will only be saved if they are callable objects and not if they are functions or lambdas.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">    It contains an entry for every variable in self.__dict__ which is not the dataloader. The batch size lambda</span>
<span class="sd">    function will only be saved if they are callable objects and not if they are functions or lambdas.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
    <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">):</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">state_dict</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.LinearBS" class="doc doc-heading">
          <code>LinearBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Increases the batch size by a linearly changing small multiplicative factor until the number of epochs reaches
a pre-defined milestone.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>start_factor</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number we multiply the batch size in the first epoch. The multiplication factor
changes towards end_factor in the following epochs. Default: 3.0.</p>
            </div>
          </td>
          <td>
                <code>3.0</code>
          </td>
        </tr>
        <tr>
          <td><code>end_factor</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number we multiply the batch size at the end of the linear changing process.
    Default: 1.0.</p>
            </div>
          </td>
          <td>
                <code>1.0</code>
          </td>
        </tr>
        <tr>
          <td><code>milestone</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number of steps that the scheduler increases the learning rate. Default: 5.</p>
            </div>
          </td>
          <td>
                <code>5</code>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming the base batch size is 10.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 60 if epoch == 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 50 if epoch == 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 40 if epoch == 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 30 if epoch == 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 20 if epoch == 4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 10 if epoch &gt;= 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">LinearBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">start_factor</span><span class="o">=</span><span class="mf">6.0</span><span class="p">,</span> <span class="n">end_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">milestone</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LinearBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Increases the batch size by a linearly changing small multiplicative factor until the number of epochs reaches</span>
<span class="sd">    a pre-defined milestone.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        start_factor (float): The number we multiply the batch size in the first epoch. The multiplication factor</span>
<span class="sd">            changes towards end_factor in the following epochs. Default: 3.0.</span>
<span class="sd">        end_factor (float): The number we multiply the batch size at the end of the linear changing process.</span>
<span class="sd">                Default: 1.0.</span>
<span class="sd">        milestone (int): The number of steps that the scheduler increases the learning rate. Default: 5.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; # Assuming the base batch size is 10.</span>
<span class="sd">        &gt;&gt;&gt; # bs = 60 if epoch == 0</span>
<span class="sd">        &gt;&gt;&gt; # bs = 50 if epoch == 1</span>
<span class="sd">        &gt;&gt;&gt; # bs = 40 if epoch == 2</span>
<span class="sd">        &gt;&gt;&gt; # bs = 30 if epoch == 3</span>
<span class="sd">        &gt;&gt;&gt; # bs = 20 if epoch == 4</span>
<span class="sd">        &gt;&gt;&gt; # bs = 10 if epoch &gt;= 5</span>
<span class="sd">        &gt;&gt;&gt; scheduler = LinearBS(dataloader, start_factor=6.0, end_factor=1.0, milestone=5)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">start_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">end_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">milestone</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">milestone</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">milestone</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">start_factor</span> <span class="o">&gt;</span> <span class="mf">0.0</span> <span class="ow">and</span> <span class="n">end_factor</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
        <span class="c1"># Both start_factor and end_factor are expected to be greater than 1.0, with start_factor &gt; end_factor, as this</span>
        <span class="c1"># should be a warmup process. But we do not forbid any other sound combinations.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">start_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">end_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">milestone</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">milestone</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. The current batch size is multiplied by the linear changing</span>
<span class="sd">        factor, starting from start_factor to end_factor. After the milestone is reached, the batch size is not changed</span>
<span class="sd">        anymore.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestone</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># My job is done.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span><span class="p">)</span>

        <span class="n">value_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_factor</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span>
        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="p">(</span>
                <span class="mf">1.0</span> <span class="o">+</span> <span class="n">value_range</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">milestone</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">value_range</span><span class="p">)))</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.LinearBS.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. The current batch size is multiplied by the linear changing
factor, starting from start_factor to end_factor. After the milestone is reached, the batch size is not changed
anymore.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. The current batch size is multiplied by the linear changing</span>
<span class="sd">    factor, starting from start_factor to end_factor. After the milestone is reached, the batch size is not changed</span>
<span class="sd">    anymore.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestone</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># My job is done.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span><span class="p">)</span>

    <span class="n">value_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_factor</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span>
    <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="p">(</span>
            <span class="mf">1.0</span> <span class="o">+</span> <span class="n">value_range</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">milestone</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_factor</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">value_range</span><span class="p">)))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.MultiStepBS" class="doc doc-heading">
          <code>MultiStepBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Multiplies the batch size by gamma once the number of epochs reaches one of the milestones.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>milestones</code></td>
          <td>
                <code><span title="typing.Sequence">Sequence</span>[int]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sequence of epoch indices.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming the base batch size is 10.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 10 if epoch &lt; 30</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 20 if 25 &lt;= epoch &lt; 80</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 40 if 80 &lt;= epoch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">MultiStepBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">80</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MultiStepBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Multiplies the batch size by gamma once the number of epochs reaches one of the milestones.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        milestones (Sequence[int]): Sequence of epoch indices.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; # Assuming the base batch size is 10.</span>
<span class="sd">        &gt;&gt;&gt; # bs = 10 if epoch &lt; 30</span>
<span class="sd">        &gt;&gt;&gt; # bs = 20 if 25 &lt;= epoch &lt; 80</span>
<span class="sd">        &gt;&gt;&gt; # bs = 40 if 80 &lt;= epoch</span>
<span class="sd">        &gt;&gt;&gt; scheduler = MultiStepBS(dataloader, milestones=[25, 80], gamma=2.0)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">milestones</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">milestones</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">milestones</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">([</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">milestones</span><span class="p">])</span>
        <span class="k">assert</span> <span class="n">gamma</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
        <span class="c1"># Gamma is expected to be greater than 1, but we do not forbid batch size decay.</span>
        <span class="c1"># We do not require milestones to be sorted. However, sorted looks better.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">:</span> <span class="n">Counter</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">milestones</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">finished</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns True if the scheduler has already finished its job or has exceeded the minimum or maximum batch</span>
<span class="sd">        size. Otherwise, returns False.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span><span class="p">:</span>
            <span class="c1"># Should we cache max(self.milestones)?</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. It returns the current batch size times gamma each epoch a</span>
<span class="sd">        milestone is reached, otherwise it returns the current batch size. Beware that in the event of multiple</span>
<span class="sd">        milestones with the same value, the current batch size is multiplied with gamma multiple times.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="bs_scheduler.MultiStepBS.finished" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">finished</span><span class="p">:</span> <span class="nb">bool</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns True if the scheduler has already finished its job or has exceeded the minimum or maximum batch
size. Otherwise, returns False.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.MultiStepBS.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. It returns the current batch size times gamma each epoch a
milestone is reached, otherwise it returns the current batch size. Beware that in the event of multiple
milestones with the same value, the current batch size is multiplied with gamma multiple times.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. It returns the current batch size times gamma each epoch a</span>
<span class="sd">    milestone is reached, otherwise it returns the current batch size. Beware that in the event of multiple</span>
<span class="sd">    milestones with the same value, the current batch size is multiplied with gamma multiple times.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.MultiplicativeBS" class="doc doc-heading">
          <code>MultiplicativeBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Multiply the batch size by a factor given in the specified function. Unlike
torch.optim.lr_scheduler.MultiplicativeLR, there is a single batch size for a given dataloader so only one function
should be passed as a parameter.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>bs_lambda</code></td>
          <td>
                <code><span title="typing.Callable">Callable</span>[[int], float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A function which computes a multiplicative factor given an integer
parameter epoch.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mf">1.05</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">MultiplicativeBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">bs_lambda</span><span class="o">=</span><span class="n">func</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MultiplicativeBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Multiply the batch size by a factor given in the specified function. Unlike</span>
<span class="sd">    torch.optim.lr_scheduler.MultiplicativeLR, there is a single batch size for a given dataloader so only one function</span>
<span class="sd">    should be passed as a parameter.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        bs_lambda (Callable[[int], float]): A function which computes a multiplicative factor given an integer</span>
<span class="sd">            parameter epoch.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; func = lambda epoch: 1.05</span>
<span class="sd">        &gt;&gt;&gt; scheduler = MultiplicativeBS(dataloader, bs_lambda=func)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">bs_lambda</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">callable</span><span class="p">(</span><span class="n">bs_lambda</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">bs_lambda</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">        It contains an entry for every variable in self.__dict__ which is not the dataloader. The batch size lambda</span>
<span class="sd">        function will only be saved if they are callable objects and not if they are functions or lambdas.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">):</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads the schedulers state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bs_lambda</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bs_lambda</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">bs_lambda</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. It is calculated as the current value of the batch size</span>
<span class="sd">        times the factor returned by `bs_lambda`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.MultiplicativeBS.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. It is calculated as the current value of the batch size
times the factor returned by <code>bs_lambda</code>.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. It is calculated as the current value of the batch size</span>
<span class="sd">    times the factor returned by `bs_lambda`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.MultiplicativeBS.load_state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Loads the schedulers state.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>state_dict</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>scheduler state. Should be an object returned from a call to :meth:<code>state_dict</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads the schedulers state.</span>

<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bs_lambda</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bs_lambda</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">bs_lambda</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.MultiplicativeBS.state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">state_dict</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the state of the scheduler as a :class:<code>dict</code>.</p>
<p>It contains an entry for every variable in self.<strong>dict</strong> which is not the dataloader. The batch size lambda
function will only be saved if they are callable objects and not if they are functions or lambdas.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">    It contains an entry for every variable in self.__dict__ which is not the dataloader. The batch size lambda</span>
<span class="sd">    function will only be saved if they are callable objects and not if they are functions or lambdas.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
    <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="p">,</span> <span class="n">types</span><span class="o">.</span><span class="n">FunctionType</span><span class="p">):</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;bs_lambda&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs_lambda</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">state_dict</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.OneCycleBS" class="doc doc-heading">
          <code>OneCycleBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Similar to torch.optim.lr_scheduler.OneCycleLR. Sets the batch size according to the one cycle batch size
policy, inspired from the 1cycle learning rate policy. The one cycle batch size policy decreases the batch size
from the base_batch_size to some minimum batch size and that it increases it to some maximum batch size bigger than
the base_batch_size.
This policy is inspired from the policy described in the paper <code>Super-Convergence: Very Fast Training of Neural
Networks Using Large Learning Rates</code>_. It only uses two phases (base -&gt; min, min -&gt; max) instead of the three
phases described in the paper (base -&gt; min, min -&gt; base, base -&gt; max).</p>
<p>The once cycle batch size policy changes the batch size after every batch. The step() function should be called
after a batch has been used for training. But it may also be called after every epoch and the total_steps should be
adjusted accordingly.</p>
<p>This scheduler is not chainable.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>total_steps</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The total number of steps in the cycle.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>decay_percentage</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The fraction of the cycle spend decreasing the batch size. 1 - decay_percentage will
be spent increasing the batch size. Default: 0.3.</p>
            </div>
          </td>
          <td>
                <code>0.3</code>
          </td>
        </tr>
        <tr>
          <td><code>base_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The base batch size. If None, the base batch size will be retrieved from
the dataloader. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>strategy</code></td>
          <td>
                <code>str</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>One of <code>cos</code>, <code>linear</code>. Specifies the strategy used for annealing the batch size, 'cos' for
cosine annealing, 'linear' for linear annealing. Default: 'cos'.</p>
            </div>
          </td>
          <td>
                <code>&#39;cos&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">OneCycleBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">total_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">train_batch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>
      <p>.. _Super-Convergence\: Very Fast Training of Neural Networks Using Large Learning Rates:
    https://arxiv.org/abs/1708.07120</p>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">OneCycleBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Similar to torch.optim.lr_scheduler.OneCycleLR. Sets the batch size according to the one cycle batch size</span>
<span class="sd">    policy, inspired from the 1cycle learning rate policy. The one cycle batch size policy decreases the batch size</span>
<span class="sd">    from the base_batch_size to some minimum batch size and that it increases it to some maximum batch size bigger than</span>
<span class="sd">    the base_batch_size.</span>
<span class="sd">    This policy is inspired from the policy described in the paper `Super-Convergence: Very Fast Training of Neural</span>
<span class="sd">    Networks Using Large Learning Rates`_. It only uses two phases (base -&gt; min, min -&gt; max) instead of the three</span>
<span class="sd">    phases described in the paper (base -&gt; min, min -&gt; base, base -&gt; max).</span>

<span class="sd">    The once cycle batch size policy changes the batch size after every batch. The step() function should be called</span>
<span class="sd">    after a batch has been used for training. But it may also be called after every epoch and the total_steps should be</span>
<span class="sd">    adjusted accordingly.</span>

<span class="sd">    This scheduler is not chainable.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        total_steps (int): The total number of steps in the cycle.</span>
<span class="sd">        decay_percentage (float): The fraction of the cycle spend decreasing the batch size. 1 - decay_percentage will</span>
<span class="sd">            be spent increasing the batch size. Default: 0.3.</span>
<span class="sd">        base_batch_size (Union[int, None]): The base batch size. If None, the base batch size will be retrieved from</span>
<span class="sd">            the dataloader. Default: None.</span>
<span class="sd">        strategy (str): One of `cos`, `linear`. Specifies the strategy used for annealing the batch size, &#39;cos&#39; for</span>
<span class="sd">            cosine annealing, &#39;linear&#39; for linear annealing. Default: &#39;cos&#39;.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">            &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">            &gt;&gt;&gt; scheduler = OneCycleBS(dataloader, total_steps=1000)</span>
<span class="sd">            &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">            &gt;&gt;&gt;     for batch in dataloader:</span>
<span class="sd">            &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">            &gt;&gt;&gt;         scheduler.step()</span>

<span class="sd">    .. _Super-Convergence\\: Very Fast Training of Neural Networks Using Large Learning Rates:</span>
<span class="sd">        https://arxiv.org/abs/1708.07120</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">decay_percentage</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
                 <span class="n">base_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;cos&#39;</span><span class="p">,</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">total_steps</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">decay_percentage</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">decay_percentage</span> <span class="o">&lt;</span> <span class="mi">1</span>
        <span class="k">assert</span> <span class="n">rint</span><span class="p">(</span><span class="n">total_steps</span> <span class="o">*</span> <span class="n">decay_percentage</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">total_steps</span> <span class="o">-</span> <span class="n">rint</span><span class="p">(</span><span class="n">total_steps</span> <span class="o">*</span> <span class="n">decay_percentage</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">base_batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">base_batch_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">base_batch_size</span> <span class="o">&gt;</span> <span class="n">min_batch_size</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;cos&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">end_step_1</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">rint</span><span class="p">(</span><span class="n">total_steps</span> <span class="o">*</span> <span class="n">decay_percentage</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_step_2</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">total_steps</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_step_1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">strategy</span>
        <span class="k">if</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s1">&#39;cos&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">anneal_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_annealing_cos</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">anneal_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_annealing_linear</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">_base_batch_size</span> <span class="k">if</span> <span class="n">base_batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">base_batch_size</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_annealing_cos</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">percentage</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Cosine annealing from start to end as percentage goes from 0.0 to 1.0.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">end</span> <span class="o">+</span> <span class="p">(</span><span class="n">start</span> <span class="o">-</span> <span class="n">end</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">))</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_annealing_linear</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">percentage</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Linear annealing from start to end as percentage goes from 0.0 to 1.0.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="n">percentage</span> <span class="o">+</span> <span class="n">start</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. Increases the batch size from base batch size to maximum</span>
<span class="sd">        batch and restarts. The implementation is similar to</span>
<span class="sd">        torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, but instead of `eta_min` we use max_batch_size, and we</span>
<span class="sd">        increase the batch size instead of decreasing the learning rate. We clip the values to always remain within</span>
<span class="sd">        bound.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Don&#39;t do anything at initialization.</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_step_1</span><span class="p">:</span>
            <span class="c1"># Phase 1</span>
            <span class="n">percentage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_step_1</span>
            <span class="n">new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">anneal_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">,</span> <span class="n">percentage</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Phase 2</span>
            <span class="n">percentage</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_step_1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_step_2</span>
            <span class="n">new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">anneal_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">percentage</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">percentage</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="n">clip</span><span class="p">(</span><span class="n">rint</span><span class="p">(</span><span class="n">new_bs</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.OneCycleBS.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. Increases the batch size from base batch size to maximum
batch and restarts. The implementation is similar to
torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, but instead of <code>eta_min</code> we use max_batch_size, and we
increase the batch size instead of decreasing the learning rate. We clip the values to always remain within
bound.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. Increases the batch size from base batch size to maximum</span>
<span class="sd">    batch and restarts. The implementation is similar to</span>
<span class="sd">    torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, but instead of `eta_min` we use max_batch_size, and we</span>
<span class="sd">    increase the batch size instead of decreasing the learning rate. We clip the values to always remain within</span>
<span class="sd">    bound.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Don&#39;t do anything at initialization.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_step_1</span><span class="p">:</span>
        <span class="c1"># Phase 1</span>
        <span class="n">percentage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_step_1</span>
        <span class="n">new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">anneal_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">,</span> <span class="n">percentage</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Phase 2</span>
        <span class="n">percentage</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_step_1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_step_2</span>
        <span class="n">new_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">anneal_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">percentage</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">percentage</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">return</span> <span class="n">clip</span><span class="p">(</span><span class="n">rint</span><span class="p">(</span><span class="n">new_bs</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.PolynomialBS" class="doc doc-heading">
          <code>PolynomialBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Increases the batch size using a polynomial function in the given total_iters. Unlike
torch.optim.lr_scheduler.PolynomialLR whose polynomial factor decays from 1.0 to 0.5 ** power, in this case the
polynomial factor decays from 1.5 ** power to 1.0.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>total_iters</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number of steps that the scheduler increases the batch size.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>power</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The power of the polynomial. Default: 1.0.</p>
            </div>
          </td>
          <td>
                <code>1.0</code>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming the base batch size is 10.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 10 if epoch == 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 10 * 1.25 if epoch == 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 12 * 1.33 if epoch == 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 16 * 1.50 if epoch == 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 24 * 2.00 if epoch == 4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 48 if epoch &gt;= 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">PolynomialBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">total_iters</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">PolynomialBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Increases the batch size using a polynomial function in the given total_iters. Unlike</span>
<span class="sd">    torch.optim.lr_scheduler.PolynomialLR whose polynomial factor decays from 1.0 to 0.5 ** power, in this case the</span>
<span class="sd">    polynomial factor decays from 1.5 ** power to 1.0.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        total_iters (int): The number of steps that the scheduler increases the batch size.</span>
<span class="sd">        power (float): The power of the polynomial. Default: 1.0.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; # Assuming the base batch size is 10.</span>
<span class="sd">        &gt;&gt;&gt; # bs = 10 if epoch == 0</span>
<span class="sd">        &gt;&gt;&gt; # bs = 10 * 1.25 if epoch == 1</span>
<span class="sd">        &gt;&gt;&gt; # bs = 12 * 1.33 if epoch == 2</span>
<span class="sd">        &gt;&gt;&gt; # bs = 16 * 1.50 if epoch == 3</span>
<span class="sd">        &gt;&gt;&gt; # bs = 24 * 2.00 if epoch == 4</span>
<span class="sd">        &gt;&gt;&gt; # bs = 48 if epoch &gt;= 5</span>
<span class="sd">        &gt;&gt;&gt; scheduler = PolynomialBS(dataloader, total_iters=5, power=1.0)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">total_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">power</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">total_iters</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">total_iters</span> <span class="o">&gt;</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">total_iters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">power</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">power</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. From epoch 1 to total_iters - 1, the current batch size is</span>
<span class="sd">        multiplied by an increasing polynomial factor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

        <span class="n">remaining_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">-</span> <span class="p">((</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">remaining_steps</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
                <span class="mf">1.0</span> <span class="o">-</span> <span class="p">(</span><span class="n">remaining_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">))</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">power</span>
        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">factor</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.PolynomialBS.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. From epoch 1 to total_iters - 1, the current batch size is
multiplied by an increasing polynomial factor.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. From epoch 1 to total_iters - 1, the current batch size is</span>
<span class="sd">    multiplied by an increasing polynomial factor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="n">remaining_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span>
    <span class="n">factor</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">-</span> <span class="p">((</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">remaining_steps</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
            <span class="mf">1.0</span> <span class="o">-</span> <span class="p">(</span><span class="n">remaining_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iters</span><span class="p">))</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">power</span>
    <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">factor</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.SequentialBS" class="doc doc-heading">
          <code>SequentialBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Similar to torch.optim.lr_scheduler.SequentialLR. Receives a sequence of schedulers and calls them sequentially
given the milestone points that reflect which scheduler is supposed to be called at a fiven epoch</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>schedulers</code></td>
          <td>
                <code><span title="typing.Sequence">Sequence</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sequence of batch size schedulers. We expect the first scheduler to have
been initialized first.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>milestones</code></td>
          <td>
                <code><span title="typing.Sequence">Sequence</span>[int]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Sequence of integers that reflects the milestone points. Must be sorted in a
non-descending order.</p>
            </div>
          </td>
          <td>
                <code><span title="typing.Sequence">Sequence</span>[int]</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming the base batch size is 10.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 100 if epoch == 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 100 if epoch == 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 100 if epoch == 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 100 if epoch == 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 10 if epoch == 4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 11 if epoch == 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler1</span> <span class="o">=</span> <span class="n">ConstantBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">milestone</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler2</span> <span class="o">=</span> <span class="n">ExponentialBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">SequentialBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">schedulers</span><span class="o">=</span><span class="p">[</span><span class="n">scheduler1</span><span class="p">,</span> <span class="n">scheduler2</span><span class="p">],</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SequentialBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Similar to torch.optim.lr_scheduler.SequentialLR. Receives a sequence of schedulers and calls them sequentially</span>
<span class="sd">    given the milestone points that reflect which scheduler is supposed to be called at a fiven epoch</span>

<span class="sd">    Args:</span>
<span class="sd">        schedulers (Sequence[BSScheduler]): Sequence of batch size schedulers. We expect the first scheduler to have</span>
<span class="sd">            been initialized first.</span>
<span class="sd">        milestones (Sequence[int]): Sequence of integers that reflects the milestone points. Must be sorted in a</span>
<span class="sd">            non-descending order.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; # Assuming the base batch size is 10.</span>
<span class="sd">        &gt;&gt;&gt; # bs = 100 if epoch == 0</span>
<span class="sd">        &gt;&gt;&gt; # bs = 100 if epoch == 1</span>
<span class="sd">        &gt;&gt;&gt; # bs = 100 if epoch == 2</span>
<span class="sd">        &gt;&gt;&gt; # bs = 100 if epoch == 3</span>
<span class="sd">        &gt;&gt;&gt; # bs = 10 if epoch == 4</span>
<span class="sd">        &gt;&gt;&gt; # bs = 11 if epoch == 5</span>
<span class="sd">        &gt;&gt;&gt; # ...</span>
<span class="sd">        &gt;&gt;&gt; scheduler1 = ConstantBS(dataloader, factor=10, milestone=4)</span>
<span class="sd">        &gt;&gt;&gt; scheduler2 = ExponentialBS(dataloader, gamma=1.1)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = SequentialBS(dataloader, schedulers=[scheduler1, scheduler2], milestones=[5])</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schedulers</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">BSScheduler</span><span class="p">],</span> <span class="n">milestones</span><span class="o">=</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schedulers</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
            <span class="p">[</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">BSScheduler</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">schedulers</span><span class="p">])</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">milestones</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">milestones</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
            <span class="p">[</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">milestones</span><span class="p">])</span> <span class="ow">and</span> <span class="n">milestones</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">milestones</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">milestones</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">milestones</span><span class="p">))]),</span> \
            <span class="sa">f</span><span class="s2">&quot;Milestones must be sorted, are </span><span class="si">{</span><span class="n">milestones</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">milestones</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SequentialBS expects the number of schedulers provided to be one more than the number &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;of milestone points, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span><span class="si">}</span><span class="s2"> and the number of milestones is &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">milestones</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span>
                         <span class="n">schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dataloader</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SequentialBS expects all schedulers to belong to the same dataloader, but got &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;scheduler at index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> to be different than the scheduler at index 0.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SequentialBS expects all schedulers to have the same batch size manager, but got &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;scheduler at index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> to have a different batch size manager. Expected type of &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;batch size manager: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">, got: &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">batch_size_manager</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_batch_size</span> <span class="o">=</span> <span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">max_batch_size</span>
            <span class="k">if</span> <span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">min_batch_size</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">min_batch_size</span> <span class="o">=</span> <span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">min_batch_size</span>

            <span class="c1"># Undoing the steps done by the schedulers.</span>
            <span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_last_bs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">_base_batch_size</span>
            <span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">-=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">set_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">_base_batch_size</span><span class="p">)</span>  <span class="c1"># Set the batch size back to initial value.</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">BSScheduler</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">schedulers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">milestones</span><span class="p">)</span>
        <span class="c1"># Do the initial step again, but only for the first scheduler.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">finished</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns True if all the schedulers have already finished their job or have exceeded the minimum or maximum</span>
<span class="sd">        batch size. Otherwise, returns False.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span><span class="p">:</span>
            <span class="c1"># The last milestone was reached and the last scheduler is finished.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">finished</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_finished</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Performs the step method for each scheduler until a milestone point is reached and a new scheduler is to be</span>
<span class="sd">        used. The new scheduler is used as if it is called for the first time.</span>

<span class="sd">        Args:</span>
<span class="sd">            **kwargs: All kwargs are passed to each scheduler.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># We still increase last_epoch, even though the scheduler has finished its job. It should</span>
        <span class="c1"># not really matter.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">bisect_right</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_last_bs</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">last_bs</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">        It contains an entry for every variable in self.__dict__ which is not the dataloader. The wrapped scheduler</span>
<span class="sd">        states will also be saved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">):</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Loads the schedulers state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">schedulers</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;schedulers&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">schedulers</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">schedulers</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">set_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_bs</span><span class="p">)</span>  <span class="c1"># Setting the batch size to the last computed batch size.</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">




<h3 id="bs_scheduler.SequentialBS.finished" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">finished</span><span class="p">:</span> <span class="nb">bool</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns True if all the schedulers have already finished their job or have exceeded the minimum or maximum
batch size. Otherwise, returns False.</p>
  </div>

</div>




<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.SequentialBS.load_state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Loads the schedulers state.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>state_dict</code></td>
          <td>
                <code>dict</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>scheduler state. Should be an object returned from a call to :meth:<code>state_dict</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Loads the schedulers state.</span>

<span class="sd">    Args:</span>
<span class="sd">        state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">schedulers</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;schedulers&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">schedulers</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">schedulers</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">set_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_bs</span><span class="p">)</span>  <span class="c1"># Setting the batch size to the last computed batch size.</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.SequentialBS.state_dict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">state_dict</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the state of the scheduler as a :class:<code>dict</code>.</p>
<p>It contains an entry for every variable in self.<strong>dict</strong> which is not the dataloader. The wrapped scheduler
states will also be saved.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the state of the scheduler as a :class:`dict`.</span>

<span class="sd">    It contains an entry for every variable in self.__dict__ which is not the dataloader. The wrapped scheduler</span>
<span class="sd">    states will also be saved.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
    <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">):</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;schedulers&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">state_dict</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.SequentialBS.step" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">step</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Performs the step method for each scheduler until a milestone point is reached and a new scheduler is to be
used. The new scheduler is used as if it is called for the first time.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>**kwargs</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>All kwargs are passed to each scheduler.</p>
            </div>
          </td>
          <td>
                <code>{}</code>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Performs the step method for each scheduler until a milestone point is reached and a new scheduler is to be</span>
<span class="sd">    used. The new scheduler is used as if it is called for the first time.</span>

<span class="sd">    Args:</span>
<span class="sd">        **kwargs: All kwargs are passed to each scheduler.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># We still increase last_epoch, even though the scheduler has finished its job. It should</span>
    <span class="c1"># not really matter.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">bisect_right</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedulers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">milestones</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">:</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_bs</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">last_bs</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h2 id="bs_scheduler.StepBS" class="doc doc-heading">
          <code>StepBS</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BSScheduler" href="#bs_scheduler.BSScheduler">BSScheduler</a></code></p>

  
      <p>Multiplies the batch size by gamma every step_size epochs.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>dataloader</code></td>
          <td>
                <code><span title="torch.utils.data.DataLoader">DataLoader</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Wrapped dataloader.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>step_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Period of batch size growth.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>gamma</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Multiplicative factor of batch size growth. Default: 2.0.</p>
            </div>
          </td>
          <td>
                <code>2.0</code>
          </td>
        </tr>
        <tr>
          <td><code>batch_size_manager</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="bs_scheduler.batch_size_schedulers.BatchSizeManager" href="#bs_scheduler.BatchSizeManager">BatchSizeManager</a>, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If not None, a custom class which manages the batch size,
which provides a getter and setter for the batch size. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>max_batch_size</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[int, None]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Upper limit for the batch size so that a batch of size max_batch_size fits
in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size
is set to <code>len(self.dataloader.dataset)</code>. Default: None.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>min_batch_size</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Lower limit for the batch size which must be greater than 0. Default: 1.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
        <tr>
          <td><code>verbose</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming the base batch size is 10.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 10 if epoch &lt; 30</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 20 if 30 &lt;= epoch &lt; 60</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># bs = 40 if 60 &lt;= epoch &lt; 90</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">StepBS</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

            <details class="quote">
              <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">StepBS</span><span class="p">(</span><span class="n">BSScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Multiplies the batch size by gamma every step_size epochs.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataloader (DataLoader): Wrapped dataloader.</span>
<span class="sd">        step_size (int): Period of batch size growth.</span>
<span class="sd">        gamma (float): Multiplicative factor of batch size growth. Default: 2.0.</span>
<span class="sd">        batch_size_manager (Union[BatchSizeManager, None]): If not None, a custom class which manages the batch size,</span>
<span class="sd">            which provides a getter and setter for the batch size. Default: None.</span>
<span class="sd">        max_batch_size (Union[int, None]): Upper limit for the batch size so that a batch of size max_batch_size fits</span>
<span class="sd">            in the memory. If None or greater than the lenght of the dataset wrapped by the dataloader, max_batch_size</span>
<span class="sd">            is set to `len(self.dataloader.dataset)`. Default: None.</span>
<span class="sd">        min_batch_size (int): Lower limit for the batch size which must be greater than 0. Default: 1.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for each update. Default: ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = ...</span>
<span class="sd">        &gt;&gt;&gt; # Assuming the base batch size is 10.</span>
<span class="sd">        &gt;&gt;&gt; # bs = 10 if epoch &lt; 30</span>
<span class="sd">        &gt;&gt;&gt; # bs = 20 if 30 &lt;= epoch &lt; 60</span>
<span class="sd">        &gt;&gt;&gt; # bs = 40 if 60 &lt;= epoch &lt; 90</span>
<span class="sd">        &gt;&gt;&gt; # ...</span>
<span class="sd">        &gt;&gt;&gt; scheduler = StepBS(dataloader, step_size=30, gamma=2.0)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(100):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">step_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
                 <span class="n">batch_size_manager</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">BatchSizeManager</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">step_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">step_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">gamma</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
        <span class="c1"># Gamma is expected to be greater than 1, but we do not forbid batch size decay.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">step_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">batch_size_manager</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="p">,</span> <span class="n">min_batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. It returns the current batch size times gamma each</span>
<span class="sd">        step_size epochs, otherwise it returns the current batch size.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="bs_scheduler.StepBS.get_new_bs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_new_bs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the next batch size as an :class:<code>int</code>. It returns the current batch size times gamma each
step_size epochs, otherwise it returns the current batch size.</p>

          <details class="quote">
            <summary>Source code in <code>bs_scheduler\batch_size_schedulers.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_new_bs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the next batch size as an :class:`int`. It returns the current batch size times gamma each</span>
<span class="sd">    step_size epochs, otherwise it returns the current batch size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">rint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": ".", "features": [], "search": "assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.aecac24b.min.js"></script>
      
    
  </body>
</html>